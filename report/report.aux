\relax 
\citation{ekf}
\citation{autocalib}
\citation{ekf}
\citation{lim}
\citation{ekf}
\citation{derotation}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\citation{scara}
\@writefile{toc}{\contentsline {section}{\numberline {2}Method for Direction of Motion Estimation}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Omnidirectional Camera Model}{3}}
\newlabel{equ:cameraModel1}{{1}{3}}
\newlabel{equ:cameraModel2}{{2}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Training images used for camera model calibration} - The orientation and position of the chessboard were changed from one image to another to most of the field of view. The images are given in grayscale by the camera and exhibit a resolution of $160 \times 120$ pixels.\relax }}{4}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:cameraImages}{{1}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Reprojection on the camera image after calibration}\relax }}{5}}
\newlabel{fig:cameraReproj}{{2}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Sampled pixels and projection on the unit sphere} - 81 pixels are uniformly sampled on a 160x120 grid (left) and back-projected on the unit sphere using the camera model (right).\relax }}{6}}
\newlabel{fig:pixelMapping}{{3}{6}}
\citation{backproj}
\newlabel{code:cameraModel}{{1}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Mapping Optical Flow to Unit Sphere}{7}}
\newlabel{code:flowProjection}{{2}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Sampling points and corresponding optic-flow vectors} - The optic-flow vectors were obtained for a straight motion along the $X$ axis.\relax }}{10}}
\newlabel{fig:flowMapping}{{4}{10}}
\newlabel{code:fastFlowProjection}{{3}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Derotation of Optical Flow}{11}}
\citation{derotation}
\citation{autocalib}
\citation{lim}
\newlabel{equ:opticflow}{{8}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Finding the Focus of Expansion}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {Great circles intersection and direction of motion} - The great circles were obtained from the sampling directions and the translational part of the optic-flow vectors. One of their intersection corresponds to the focus of expansion. \relax }}{13}}
\newlabel{fig:voting}{{5}{13}}
\newlabel{equ:normalVector}{{11}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textbf  {Normal vector and its corresponding great circle} - The normal vector is computed using Equation\nobreakspace  {}11\hbox {}. The great circle is plotted using its parametric equation: $v(t) = cos(t) \cdot \mathaccentV {hat}05E{f}_t + sin(t) \cdot d$, where $\mathaccentV {hat}05E{f}_t = \genfrac  {}{}{}0{f_t}{\delimiter "026B30D f_t\delimiter "026B30D }$. \relax }}{14}}
\newlabel{fig:normalVector}{{6}{14}}
\citation{rotation}
\newlabel{algo:houghVoting}{{1}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \textbf  {Refined bins before and after rotation} - The refined bins, centered around $z<0$, are stored (left) before being rotated to be centered around the latest direction estimate (right).\relax }}{15}}
\newlabel{fig:refinedBins}{{7}{15}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \textbf  {Voting bins parameters} - The voting bins are evely distributed on the unit sphere and were generated using icosahedron method.\relax }}{16}}
\newlabel{tab:votingBins}{{1}{16}}
\newlabel{algo:rotation}{{4}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results \& Discussion}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Simulation on Matlab}{18}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \textbf  {Simulation parameters} - The velocity and angular rates were divided by the frequency of the camera (200Hz) to emulate real optic-flow computation\relax }}{18}}
\newlabel{tab:simParam}{{2}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces \textbf  {Generated sampling points and optic-flow vectors} - Synthesized using the first-order approximation and the parameters detailed in Table\nobreakspace  {}2\hbox {}\relax }}{19}}
\newlabel{fig:simOpticFlow}{{8}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces \textbf  {Generated outliers} - Synthesized using the first-order approximation and the parameters detailed in Table\nobreakspace  {}2\hbox {} and additional randomly generated tangent vectors\relax }}{19}}
\newlabel{fig:simOutliers}{{9}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces \textbf  {Derotated optic-flow vectors} - Synthesized using the first-order approximation and the parameters detailed in Table\nobreakspace  {}2\hbox {} and additional randomly generated tangent vectors\relax }}{20}}
\newlabel{fig:simDerotation}{{10}{20}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces \textbf  {Influence of outliers on estiamte} - The dot products of the estimated and real directions of motion are averaged over 100 runs with various proportions of outliers\relax }}{20}}
\newlabel{tab:simResults}{{3}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Experiments with motion capture}{20}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Setup}{20}}
\citation{adapted_neighborhood}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces \textbf  {Optic-flow sensor mounted on its motion capture frame} - The frame is equipped with 5 passive markers attached at known positions to obtain 3D position and orientation of the rigid body out of the 21 IR cameras.\relax }}{21}}
\newlabel{fig:motionCapture}{{11}{21}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Results}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces \textbf  {Synchronized signals from camera and optitrack} - The signals were synchronized manually using recognizable features. The axes of the optitrack were circularly permutated so as to correspond to the ones used in the camera frame. The horizontal axis correspond to the index of the frame ($\sim $5ms)\relax }}{23}}
\newlabel{fig:mocapGyro81}{{12}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces \textbf  {Synchronized signals from camera and optitrack} - The signals were synchronized manually using recognizable features. The axes of the optitrack were circularly permutated so as to correspond to the ones used in the camera frame. The horizontal axis correspond to the index of the frame ($\sim $5ms)\relax }}{23}}
\newlabel{fig:mocapGyro117}{{13}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces \textbf  {Comparison of optic-flow sensor estimate with "ground-truth"} - The horizontal axis correspond to the index of the frame ($\sim $5ms) \relax }}{24}}
\newlabel{fig:mocapPos81}{{14}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces \textbf  {Comparison of optic-flow sensor estimate with "ground-truth" (cont'd)} - A gaussian window ($\sigma = 20ms$) was applied. The horizontal axis correspond to the index of the frame (\nobreakspace  {}5ms) \relax }}{25}}
\newlabel{fig:mocapPos81_sig4}{{15}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces \textbf  {Comparison of optic-flow sensor estimate with "ground-truth" (cont'd)} - A gaussian window ($\sigma = 100ms$) was applied. The horizontal axis correspond to the index of the frame (\nobreakspace  {}5ms) \relax }}{26}}
\newlabel{fig:mocapPos81_sig20}{{16}{26}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion}{27}}
\bibstyle{ieeetr}
\bibdata{report}
\bibcite{ekf}{1}
\bibcite{autocalib}{2}
\bibcite{lim}{3}
\bibcite{derotation}{4}
\bibcite{scara}{5}
\bibcite{backproj}{6}
\bibcite{rotation}{7}
\bibcite{adapted_neighborhood}{8}
