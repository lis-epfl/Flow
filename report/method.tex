\section{Method for Direction of Motion Estimation}	
\subsection{Omnidirectional Camera Model}

All modern fisheye cameras are central, and hence, they satisfy the single effective focal point property. The reason a single effective viewpoint is so desirable is that it allows us to generate geometrically correct perspective images from the pictures captured by the omnidirectional camera. When the geometry of the omnidirectional camera is known, that is, when the camera is calibrated, one can precompute this direction for each pixel. Therefore, each pixel can be mapped onto a plane at any distance from the viewpoint to form a planar perspective image. Additionally, the image can be mapped onto a sphere centered on the single viewpoint, that is, spherical projection.

Omnidirectional camera systems cannot be described using conventional pinhole model because of the very high distortion induced by the imaging device. Indeed, in our case, the model should take into account the multiple refractions caused by the lenses of the fisheye camera. A unified model was proposed in \cite{scara}. To overcome the lack of knowledge of a parametric model for fisheye cameras, a Taylor polynomial, whose coefficients and degree are found through the calibration procedure, is used.

Let $p$ be a pixel point of your image, and $(u,v)$ its pixel coordinates with respect to the center of the omnidirectional image. Let $P$ be its corresponding 3D vector emanating from the single effective viewpoint, and $(x,y,z)$ its coordinates with respect to the axis origin. The function estimated by the calibration process maps an image point $p$ into its corresponding 3D vector $P$:\\
\begin{equation}
\label{equ:cameraModel1}
P = \begin{bmatrix}
		x\\
		y\\
		z\\
	\end{bmatrix}
	= \begin{bmatrix}
		u\\
		v\\
		f(r)\\
	  \end{bmatrix}
\end{equation}
where 
$
\begin{cases}
r = \sqrt{u^2 + v^2}\\
f(r)= a_0 + a_1r + a_2r^2 + a_3r^3 + a_4r^4 + ...\\
\end{cases}
$\\
and the parameters to estimate are $a_0$, $a_1$, $a_2$, ...
Although increasing the polynomial may yield better accuracy, we used 4\textsuperscript{th} order polynomials, as a good trade-off between accuracy and complexity of the polynomial model.

However, as the camera and lenses axes are never perfectly aligned, the model is extended so as to model these errors through an affine transformation:
\begin{equation}
\label{equ:cameraModel2}
\begin{bmatrix}
u'\\
v'\\
\end{bmatrix}
=
\begin{bmatrix}
c & d\\
e & 1\\
\end{bmatrix}
\cdot
\begin{bmatrix}
u\\
v\\
\end{bmatrix}
+
\begin{bmatrix}
x_c\\
y_c\\
\end{bmatrix}
\end{equation}
which relates the real distorted coordinates $(u', v')$ to the ideal undistorted ones $(u,v)$.

This approximate model of the camera allows for more scalability in the number of optical flow measurements and more flexibility on their location, without the need to proceed to another calibration. Furthermore, the viewing directions corresponding to each pixel can be pre-computed as part of an initialization procedure to speed up subsequent computations.

The proposed calibration procedure relies on the use of a chessboard to automatically locate feature points on the camera images. Unfortunately, the resolution of our camera ($160 \times 120$) was too low for the edge detection algorithm to perform correctly its task. This issue may be avoided by using circles instead of the squares of the checkerboard. Indeed, this would yield more easily distinguishable corners. Considering this was not part of the current toolbox, we simply located 35 corners manually for each image, prior to calibration. We used 7 images taken from the camera with varying positions and orientation of the chessboard so as to cover most of the its field of view (Fig.~\ref{fig:cameraImages}). 

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/camera/0.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/camera/1.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/camera/2.png}
    \end{subfigure}
	\begin{subfigure}[b]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/camera/3.png}
    \end{subfigure}
	\begin{subfigure}[b]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/camera/4.png}
    \end{subfigure}
	\begin{subfigure}[b]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/camera/5.png}
    \end{subfigure}
	\begin{subfigure}[b]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/camera/6.png}
    \end{subfigure}
    \caption{\textbf{Training images used for camera model calibration} - The orientation and position of the chessboard were changed from one image to another to most of the field of view. The images are given in grayscale by the camera and exhibit a resolution of $160 \times 120$ pixels.}
    \label{fig:cameraImages}
\end{figure}

In the end, we obtained a subpixel average error of $0.34$ pixels and the following model, which relies on a 4\textsuperscript{th}-order Taylor polynomial:\\
\begin{center}
Polynomial:
$
\begin{cases}
a_0 = -6.66 . 10^{1}\\ 
a_1 = 0.00 \\
a_2 = 6.42 . 10^{-3} \\
a_3 = -2.31 . 10^{-5} \\
a_4 = 2.73 . 10^{-7} \\
\end{cases}
$
\hfill
Center:
$
\begin{cases}
x_c = 56.23 \\
y_c = 77.64 \\
\end{cases}
$
\hfill
Matrix:
$
\begin{cases}
c = 1.00 \\
d = -0.00 \\
e = -0.00 \\
\end{cases}
$
\end{center}
As we can see from the coefficients of the matrix in (\ref{equ:cameraModel2}), there is not significant misalignment between the camera and the lenses. Hence, we can neglect this part of the model if necessary. Nevertheless, the estimated location of the center is not exactly the actual center of the $160 \times 120$ grid of pixels. The reprojections of the points used for calibration appear on Figure~\ref{fig:cameraReproj}. 

This model was implemented as a structure which is passed as parameters to any function that needs it. The implementation of the projection from camera image to the (3D) scene is shown in Code~\ref{code:cameraModel}.
\newpage

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/camera/Reproj7.eps}
    \end{subfigure}
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/camera/Reproj6.eps}
    \end{subfigure}
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/camera/Reproj5.eps}
    \end{subfigure}
	\begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/camera/Reproj4.eps}
    \end{subfigure}
	\begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/camera/Reproj3.eps}
    \end{subfigure}
	\begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/camera/Reproj2.eps}
    \end{subfigure}
	\begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/camera/Reproj1.eps}
    \end{subfigure}
    \caption{\textbf{Reprojection on the camera image after calibration}}
    \label{fig:cameraReproj}
\end{figure}
\newpage

\begin{center}
\begin{code}[colback=white, label=code:cameraModel]{From pixels to 3D directions}
void cam2world(float *xp, float *yp, float *zp, float u, 
		float v, cam_model *cam)
{
	 float *pol    = cam->pol;
	 float xc      = cam->xc;
	 float yc      = cam->yc; 
	 float c       = cam->c;
	 float d       = cam->d;
	 float e       = cam->e;
	 uint8_t length_pol = cam->length_pol;

	 float invdet  = 1/(c-d*e);

	 // back-projection of u and v
	 *xp = invdet*(    (u - xc) - d*(v - yc) );
	 *yp = invdet*( -e*(u - xc) + c*(v - yc) );
	  
	 float r = sqrt(SQR((*xp)) + SQR((*yp)));
	 *zp  	 = pol[0];
	 
	 float r_i = 1;
	 
	 // compute z from polynomial model
	 for (uint8_t i = 1; i < length_pol; i++)
	 {
	   r_i *= r;
	   *zp += r_i*pol[i];
	 }
}
\end{code}
\end{center}

\subsection{Mapping Optical Flow to Unit Sphere}
Now that the model of the omnidirectional camera is known - and the pixels can be mapped onto a plane or a sphere -, we can deduce the viewing direction corresponding to a given pixel. However, the particular geometry of the omnidirectional camera induces high distortions in the image. Hence, the optical flow, which is computed using Lucas-Kanade method, can be projected onto the unit sphere using the method presented in \cite{backproj}. Mapping the optical flow directly on the unit sphere allows us to simplify further processing required for the estimation of the location of the focus of expansion, through the straightforward use of spherical coordinates. However, though more natural, the sphere still induces radial distortions and may not be appropriate for all types of motions, as exposed by Shakernia et al.

The optical rays are described in spherical coordinates $(\rho, \theta, \Phi)$ from the center of the camera, where $\rho$ is the magnitude, $\theta$ is the azimuth in the $X$-$Y$ plane, and $\Phi$ is the polar angle between the ray and the $Z$-axis. Given an image point $(u,v)^T$, we first compute the corresponding ray (or back-projection ray) $b=(x, y, z)^T$ using equations (\ref{equ:cameraModel1}) and (\ref{equ:cameraModel2}), and normalize it to unit length $s = b/\|b\|$. The spherical coordinates of the "unitized" back-projection ray are given by:
\begin{equation}
\begin{cases}
\rho = 1\\[6pt]
\theta = \arctan(\dfrac{y}{x})\\[6pt]
\Phi = \arctan(\dfrac{r}{z})\\[6pt]
\end{cases}
\end{equation}
where $r = \sqrt{x^2+y^2}$. Using equations (\ref{equ:cameraModel1}) and (\ref{equ:cameraModel2}), we can derive the following Jacobian which relates the partial derivatives from the image plane to the unit sphere:
\begin{equation}
J =
\begin{bmatrix}
\dfrac{\partial\theta}{\partial u} & \dfrac{\partial\theta}{\partial v}\\[8pt]
\dfrac{\partial\Phi}{\partial u}	  & \dfrac{\partial\Phi}{\partial v}\\
\end{bmatrix}
=\dfrac{1}{det(A)}
\begin{bmatrix}
\dfrac{-y - ex}{r^2} & \dfrac{dy + cx}{r^2}\\[8pt]
\dfrac{d\Phi}{dr}\dfrac{(x-ey)}{r} & \dfrac{d\Phi}{dr}\dfrac{(cy - dc)}{r}\\
\end{bmatrix}
\end{equation}
where 
$
A = \begin{bmatrix}
c & d\\
e & 1\\
\end{bmatrix}
$
and 
$ \dfrac{d\Phi}{dr} = \dfrac{z - r\rho'(r)}{r^2 + z^2}$\\
Then, we compute the transformation which takes the partial derivatives on the unit sphere from spherical coordinates to rectangular coordinates:
\begin{equation}
S =
\begin{bmatrix}
-\sin\theta \sin\Phi & \cos\theta \cos\Phi\\
~~\cos\theta \sin\Phi & \sin\theta \cos\Phi\\
0 & -\sin\Phi\\
\end{bmatrix}
\end{equation}
Therefore, we can compute the mapping of an image point $(u,v)^T$ (considered distorted) and its corresponding optical flow $(\dot{u}, \dot{v})^T$ to the unit sphere:
\begin{align}
s &= b/\|b\| & \dot{s} &= SJ \begin{bmatrix} \dot{x} \\ \dot{y} \end{bmatrix}
\end{align}
A possible C implementation is detailed in Code~\ref{code:flowProjection}

\begin{center}
\begin{code}[colback=white, label=code:flowProjection]{Backprojection of optical flow}
void flow2world(float *flow_x, float *flow_y, float *flow_z, 
		float xp, float yp, float zp, float flow_u, 
		float flow_v, cam_model *cam)
{
	 float *pol    = cam->pol;
	 float c       = cam->c;
	 float d       = cam->d;
	 float e       = cam->e;
	 uint8_t length_pol = cam->length_pol;
	 float invdet  = 1/(c-d*e); 
	 
	 // from cartesian to spherical coordinates
	 float r   = sqrt(SQR(xp) + SQR(yp));
	 float theta = atan2(yp,xp);
	 float phi = atan2(r, zp); 
	 
	 // compute polynomial model derivative
	 float r_i = 1;
	 float dzp = pol[1];
	 for (uint8_t i=2; i < length_pol; i++)
	 {
	   r_i *=r;
	   dzp += i*pol[i]*r_i;
	 }
	 // project optic-flow on unit sphere
	 float d_thetau = invdet*(-yp/SQR(r) - e*xp/SQR(r));
	 float d_thetav = invdet*(d*yp/SQR(r) + c*xp/SQR(r));
	 float d_phir = (zp - r*dzp)/(SQR(zp) + SQR(r));
	 float d_phiu = d_phir*invdet*(xp/r - e*yp/r);
	 float d_phiv = d_phir*invdet*(-d*xp/r + c*yp/r);
	 float flow_theta = d_thetau*flow_u 
	 		   + d_thetav*flow_v;
	 float flow_phi = d_phiu*flow_u + d_phiv*flow_v;
	 
	 // from spherical to cartesian coordinates
	 *flow_x = -sin(theta)*qsin(phi)*flow_theta 
	 	   + cos(theta)*cos(phi)*flow_phi;
	 *flow_y = cos(theta)*sin(phi)*flow_theta 
	 	   + sin(theta)*cos(phi)*flow_phi;
	 *flow_z = -sin(phi)*flow_phi;
}
\end{code}
\end{center}

This method is obviously quite computationally expensive. Indeed, further testing on the camera board revealed that the computation  and processing of $117$ optic-flow vectors accounts for approximately 12ms. This value is to be compared to the refresh rate of our camera which is 200Hz. Thus, the runtime (for one iteration) is more than doubled by the use of the above method. 

In an attempt to reduce the computational overhead, we considered an approximate projection of the optic-flow measurements on the unit sphere. Considering the high refresh rate of the camera, any motion will induce a low displacement of the pixels on the camera image (i.e. a small optic-flow measurement). The latter remark allows us to take advantage of the small-angle approximation. More formally, we approximate the actual optic-flow projection $F$ to the difference between a sampling direction $P_1$ and a virtual point $P_2$ that is obtained from the projection of the point $p_2 = p_1 + f$, where $p_1$ is the point on the camera image corresponding to the 3D vector $P_1$ and $f$ is the 2D optic-flow:
\begin{align}
\begin{split}
{}&P_2 = g(p_2) = g(p_1 + f)\\
{}&F \approx P_2 - P_1\\
\end{split}
\end{align}
where $g$ is the spherical projection function described in the previous section and $F$ is the spherical projection of the optic-flow.

The resulting optic-flow vector is obviously not perpendicular to the sphere. However, building on the same assumption, we can neglect the radial component of the flow and simply use this raw vector for subsequent computations. The C implementation is shown in Code~\ref{code:fastFlowProjection}.

As for computational gain, running this version of the flow projection dramatically reduces the runtime, with only half the time required by the initial algorithm to process the $117$ optic-flow measurements.

\begin{center}
\begin{code}[colback=white, label=code:fastFlowProjection]{Fast backprojection of optical flow}
void fast_flow2world(float *flow_x, float *flow_y, 
		float *flow_z, float xp1, float yp1, 
		float zp1, float u1, float v1, 
		float flow_u, float flow_v, cam_model *cam)
{
	 // add optic-flow to pixel coordinates (u1, v1)
	 float u2 = u1 + flow_u;
	 float v2 = v1 + flow_v;

	 // define new coordinates
	 float xp2;
	 float yp2;
	 float zp2;

	 // map to 3d scene
	 cam2world(&xp2,  &yp2,  &zp2,  u2,  v2, cam);

	 // project on unit sphere
	 normalize(&xp2, &yp2, &zp2);

	 // compute optic-flow considering small angles
	 *flow_x = xp2 - xp1;
	 *flow_y = yp2 - yp1;
	 *flow_z = zp2 - zp1;
}
\end{code}
\end{center}

\subsection{Derotation of Optical Flow}
Under the assumption of differential motion, that is small translation and rotation of the camera between each frame, we can approximate the geometrical effects of egomotion on the perceived image motion to a first-order Taylor polynomial. Hence, as the scene is projected on the unit sphere centered on the vantage point, each optical flow measurement can be expressed as a 3D vector tangent to the unit sphere and perpendicular to the viewing direction:
\begin{equation}
\label{equ:opticflow}
f = -w \times d - \dfrac{v - (v \cdot d)d}{D}
\end{equation}
where $d$ is a unit vector describing the viewing direction, $w$ the angular speed vector, $v$ the translational velocity vector and $D$ the distance to the viewed object. The measured optical flow $f$ can be decoupled and expressed in two parts, namely the rotation-induced and the translation-induced optical flows, respectively $f_r$ and $f_t$.

As we are only interested in the estimation of the angle of attack (i.e. the direction of translation), only the translational part $f_t$ of the optical flow is required. Hence, we proceed to the removal of the rotational part $f_r$ of the optical flow through the process of derotation \cite{derotation}. If necessary, the main axis $Z$ of the camera can be automatically calibrated using the method proposed in \cite{autocalib}, so as to estimate the transformation from the gyroscope frame to the camera frame. Even after calibration, the derotation procedure may introduce additional noise to the optical flow measurement, all the more so that the rotational component is larger than the translational part of the optical flow. This may be exacerbated by the sensor bias compensation and noise reduction only achieved using averaging and low-pass filtering.

However, before proceeding to the derotation of the optical flow, we must first consider scaling the angular rate values. Indeed, once projected on the unit sphere, the optical flow is given in [frame\textsuperscript{-1}] whereas the gyroscopes measurements are [rad.s\textsuperscript{-1}]. Given the refresh rate of the camera, the conversion is straight forward: 
\begin{equation}
w\mathrm{[frame\textsuperscript{-1}]} = 0.005 \cdot w\mathrm{[rad/s\textsuperscript{-1}]}
\end{equation}

\subsection{Finding the Focus of Expansion}
In order to find the direction of translation, we use the derotated optical flow to constrain the translation direction to lie on a great circle. This great circle is the intersection of a plane - defined by the vector defined by the viewing direction $d$, its corresponding optical flow vector $p_t$ and the center of the sphere $O$ - with the unit sphere. Hence, each optic-flow vector gives us a constrain on the position of the focus of expansion (FOE), which can be located anywhere on the great circle. Although only two great circles are required to constrain the location of the FOE to two points (and thus to find the axis of motion), several measurements are used so as to recover from possible outliers and noise.

For a robust estimation of the direction of translation, two different approaches are possible. The first one relies on RANSAC (RANdom SAmple Consensus) and its variants. However, this first method implies several estimations of the location of the FOE based on randomly sampled measurements and may not be suitable for implementation on a microcontroller and fast refresh rate. Another approach is to find the best intersection of all the great circle arising from the optical flow measurements such as the Hough-reminiscent voting method proposed in \cite{lim}. Voting has the advantage of performing in constant time under increasing outlier proportions without any loss of accuracy. Its higher robustness and the fact that it does not require an optimization process make this method the most suitable for embedded robotics. 
 
% Let $\hat{p}_t = \dfrac{p_t}{\|p_t\|}$ be the normalized optical flow vector corresponding to the viewing direction $d$. A parametric equation of the great circle defined by these two normalized vectors is:
% \begin{equation}
% v(t)= cos(t)\cdot\hat{p}_t + sin(t)\cdot d
% \end{equation}
% From this equation, we can proceed to voting along the great circle corresponding to each optical flow measurement. For the procedure not to be too computationally heavy, we can perform a coarse voting to determine a first approximation for the location of the FOE. Then, we refine our estimate by projecting the great circle on a plane tangent to the unit sphere in the vicinity of the estimated location. Hence, the voting procedure is simplified by voting along lines in the plane and there are efficient algorithms to perform this task, such as the Bresenham line algorithm.

In order to simplify the mathematical representation of the great circles, we rely on their normal vectors. Indeed, since a great circle is the intersection of the sphere with a plane passing though its origin, it can be defined using one of the unit normal vectors.
Formally, for a unit vector $n = (n_x, n_y, n_z)^T$, a great circle on the unit sphere $S^2$ is defined as
\begin{equation}
C = S^2~\bigcup~\{ x|x^Tn = 0, x \in \mathbb{R}^3 \}
\end{equation}
This normal vector $n$ is readily computed using the cross-product,
\begin{equation}
n = \dfrac{d \times f_t}{\|d \times f_t\|}
\end{equation}
% Considering two great circles and their corresponding normal vectors, namely $n_i$ and $n_j$, we can compute their intersections $a_{ij}$ and $a_{ji}$,
% \begin{equation}
% \begin{cases}
% a_{ij} = n_i \times n_j\\
% a_{ji} = n_j \times n_i\\
% \end{cases}
% \end{equation}
% Hence, we can vote to the accumulator for either $a_{ij}$ and $a_{ji}$ for each pair of points. Of course, the intersection of the two great circles actually yields two points.
Thus, we can vote along a great circle, given its normal vector, by computing simple dot products (i.e. 3 multiplications and 2 additions).

In practice, the intersection of two great circles defines 2 possible directions of translation. To disambiguate between the two, one may pick one of the two points and calculate the angle between it and the vector $f_t$. If the angle is larger than $\pi/2$ radians, then that point is the focus of expansion. Another possibility is to use an additional sensor (such as a sonar) to choose the correct direction. In order to improve the processing speed on the camera chip (and to avoid ambiguity), we only consider one hemisphere, that is half the number of possible directions of translation. In other words, we only determine the axis of translation rather than its direction and we work on the hemisphere defined by $z<0$.

The voting procedure requires the storage of evenly distributed vectors on the unit sphere. The minimal angle between these vectors define the resolution of the voting procedure. For tractability reasons, we cannot afford to vote on the complete unit sphere in one batch. Hence, we are considering an iterative method which can be divided in two phases. The first one is a coarse voting procedure where we compute a rough estimate of the direction of translation using a few directions sampled from one hemisphere. This estimate is then used in the next phase where we perform voting on a refined spherical grid of higher resolution but reduced spread. The complete voting procedure is detailed in Algorithm~\ref{algo:houghVoting}.

\begin{algorithm}[colback=white, label=algo:houghVoting]{Hough-reminiscent voting}
Select optic flow measurement $p_t$ and viewing direction $d$
Compute normal vector $n = \dfrac{d \times p_t}{\|d \times p_t\|}$
For i = 1 to $N_{coarse}$ do
   If $x_i^Tn = 0$ then
      Vote for direction $x_i$
   end if
end for
Find the bins with maximum votes. This gives the coarse estimate.
Rotate the refined bins to center them on coarse estimate
For j = 1 to $N_{refined}$ do
   If $x_j^Tn = 0$ then
      Vote for direction $x_j$
   end if
end for
Find the bins with maximum votes. This gives the fine estimate
\end{algorithm}

The refined voting requires rotating the refined bins, which we computed for the a predefined direction (in our case $z = (0,~ 0,-1)$). There are three ways to achieve this transformation, namely the rotation matrix, axis-angle and quaternion methods \cite{rotation}. From a computational point of view, the rotation matrix method is the cheapest one (6 additions and 9 multiplications) whereas the quaternion method is the most expensive (18 additions and 21 multiplications). However, it is easier for us to use the axis-angle representation and then convert it to a rotation matrix so as to perform the transformation on every bins. The latter convertion accounts for 13 additions and 15 multiplications but the subsequent processing will only require 6 additions and 9 multiplications, provided that we can store the entire rotation matrix. A C implementation is given in Code~\ref{algo:rotation}.

Obviously, this whole method is sensible to noise which could affect the voting procedure if the accumulator has a finite resolution. In a first case, the coarse resolution was set to $31^\circ$ and the fine resolution was chosen as approximately $4^\circ$. The former value was chosen so as to limit the number of coarse bins while the second one offers a trade-off between the resolution and the required computational power. In a second case, we added 3 more stages of refined voting so as to reduce the number of voting bins in each stage (i.e. reduce computational overhead) and simultaneously slightly increase the accuracy of the result. Indeed, the computational cost of the votin procedure typically grows linearly with the number of samples and the number of bins while more stages allow for a higher resolution in the refined votings.\\
The voting bins were evenly distributed on the unit and generated using the icosahedron method. Table~\ref{tab:votingBins} summarizes the voting bins parameters.

\begin{table}[h!]
	\centering
	\begin{tabular}{|c|p{2cm}||p{3cm}|p{3cm}|p{3cm}|}
	   \hline
	   \multicolumn{5}{|c|}{Voting bins parameter} \\
	   \hline
	   Stages & Voting set & Region & Number of bins & Resolution ($^\circ$) \\
	   \hline
	   \multirow{2}{*}{2} & Coarse & Hemisphere & 25 & 31\\
	   \cline{2-5}
	    & Refined & Coarse estimate & 42 & 4\\
	   \hhline{|=|=#=|=|=|}
	   \multirow{5}{*}{5} & Coarse & Hemisphere & 8 & 100\\
	   \cline{2-5}
	    & Refined & Coarse estimate & 7 & 31.7\\
	   \cline{2-5}
	    & Refined & Previous estimate & 7 & 11.4\\
	   \cline{2-5}
	    & Refined & Previous estimate & 7 & 3.4\\
	   \cline{2-5}
	    & Refined & Previous estimate & 7 & 1.6\\
	   \hline
	\end{tabular}
	\caption{\textbf{Voting bins parameters} - The voting bins are evely distributed on the unit sphere and were generated using icosahedron method.}
	\label{tab:votingBins}
\end{table}

\begin{code}[colback=white, label=algo:rotation]{Hough-reminiscent voting}
void rotate_bins(voting_bins *bins, float best_x, 
						float best_y, float best_z, 
						float *r_dir_x, float *r_dir_y, 
						float *r_dir_z)
{
	if(best_x==0.0f, best_y==0.0f, best_z==-1.0f){
		for(uint8_t i=0; i<bins->size; i++){
			bins->x[i] = r_dir_x[i];
			bins->y[i] = r_dir_y[i];
			bins->z[i] = r_dir_z[i];
		} 
	}
	else{
		// store cosine and sine
		float c = -best_z;
		float s = maths_fast_sqrt(1 - SQR(c)); 

		// store axis
		float u_x = 1.0f*best_y;					
		float u_y = -1.0f * best_x;					
		float u_z = 0.0f;
		normalize(&u_x, &u_y, &u_z);

		// compute rotation matrix
		float R[9];
		aa2mat(R, u_x, u_y, u_z, c, s);

		// compute rotated refined bins
		for(uint8_t i=0; i<bins->size; i++){
			bins->x[i] = r_dir_x[i]*R[0]+r_dir_y[i]*R[1]
							+r_dir_z[i]*R[2];
			bins->y[i] = r_dir_x[i]*R[3]+r_dir_y[i]*R[4]
							+r_dir_z[i]*R[5];
			bins->z[i] = r_dir_x[i]*R[6]+r_dir_y[i]*R[7]
							+r_dir_z[i]*R[8];
		} 
	}
}
\end{code}

\newpage